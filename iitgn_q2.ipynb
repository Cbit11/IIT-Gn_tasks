{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMhkAfANSCpJ3tONFkJuWSP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"84SHG0fPbzMH","executionInfo":{"status":"ok","timestamp":1676719744623,"user_tz":-330,"elapsed":12,"user":{"displayName":"CJ","userId":"16852632714411506132"}}},"outputs":[],"source":["import jax.numpy as jnp\n","from jax import random\n","import numpy as np "]},{"cell_type":"code","source":["# [5 Marks] Briefly explain and implement from scratch the following functions: i)\n","# cross-entropy; ii) entropy; iii) mutual information; iv) conditional entropy; v) KL\n","# divergence. Take appropriate example toy data/distributions and explain the insights\n","# from calculating these quantities"],"metadata":{"id":"cockIa64cDSD","executionInfo":{"status":"ok","timestamp":1676719744624,"user_tz":-330,"elapsed":11,"user":{"displayName":"CJ","userId":"16852632714411506132"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# 1. Cross Entropy: Cross Entropy is the measure of diffrence between 2 probability distributions. \n","\n","def cross_entropy(y, p):\n","  entropy= -jnp.sum(y* jnp.log(p)+(1-y)*jnp.log(1-p))\n","  return entropy\n","\n","x= np.random.randint(2, size=10)\n","y= jnp.array(x)\n","p= np.random.rand(10)\n","p = jnp.array(p)\n","entropy= cross_entropy(y,p)\n","entropy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"65uxXSJSdXd4","executionInfo":{"status":"ok","timestamp":1676719744624,"user_tz":-330,"elapsed":11,"user":{"displayName":"CJ","userId":"16852632714411506132"}},"outputId":"9623db3b-046c-493b-b252-148c0ccd7c0a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"]},{"output_type":"execute_result","data":{"text/plain":["DeviceArray(15.44427, dtype=float32)"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["#2. entropy: It is the average information given in probability distribution\n","def entropy(p):\n","  entropy= -np.sum(p*np.log(p))\n","  return entropy\n","p= np.random.rand(10)\n","p = jnp.array(p)\n","h= entropy(p)\n","h"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TLaRZmqlrIN6","executionInfo":{"status":"ok","timestamp":1676719744624,"user_tz":-330,"elapsed":9,"user":{"displayName":"CJ","userId":"16852632714411506132"}},"outputId":"adcb7c5e-ee8e-4647-b2f3-06fe05af3c1d"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DeviceArray(1.9130508, dtype=float32)"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["#4. conditiona entropy: It is the amount of information needed to describe the outcome of a random variable Y given that the value of another random variable X is known.\n","def conditional_entropy(X, Y):\n","    \"\"\" \n","    Calculate conditional entropy of all columns of X against Y (i.e. \\sum_i=1^{N} H(X_i | Y)).\n","    \"\"\"\n","   # Calculate distribution of y    \n","    Y_dist = np.zeros(shape=(int(Y.max()) + 1, ), dtype=np.float32)\n","    for y in range(Y.max() + 1):\n","        Y_dist[y] = (float(len(np.where(Y==y)[0]))/len(Y))\n","        \n","    Y_max = Y.max()\n","    X_max = X.max()\n","    \n","    ce_sum = 0.\n","    for i in range(X.shape[1]):\n","        ce_sum_partial = 0.\n","        \n","        # Count \n","        counts = np.zeros(shape=(X_max + 1, Y_max + 1), dtype=np.int32)\n","        for row, x in enumerate(X[:, i]):\n","            counts[x, Y[row]] += 1\n","        \n","        # For each value of y add conditional probability\n","        for y in range(Y.max() + 1):\n","            count_sum = float(counts[:, y].sum())\n","            probs = counts[:, y] / count_sum\n","            entropy = -probs * np.log2(probs)\n","            ce_sum_partial += (entropy * Y_dist[y]).sum()\n","\n","        ce_sum += ce_sum_partial\n","        \n","    return ce_sum"],"metadata":{"id":"tJyG5N9itxnR","executionInfo":{"status":"ok","timestamp":1676719744625,"user_tz":-330,"elapsed":8,"user":{"displayName":"CJ","userId":"16852632714411506132"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["X = np.random.randint(0, 2, size=(100, 10))\n","Y = np.random.randint(0, 2, size=(100))\n","val = conditional_entropy(X, Y)\n","val"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RI_celAT06op","executionInfo":{"status":"ok","timestamp":1676719744625,"user_tz":-330,"elapsed":7,"user":{"displayName":"CJ","userId":"16852632714411506132"}},"outputId":"beb8a08e-3140-4bbe-aa6a-d197d8432362"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9.875865098587873"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["#5. KL Divergence: It is a measure of how 1 probability distribution p is different from probability distribution q\n","def divergence(p, q):\n","  div= jnp.sum(p* jnp.log(p/q))\n","  return div\n","p = np.random.rand(10)\n","q= np.random.rand(10)\n","div = divergence(p, q)\n","div"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IMFvLwRgRzi1","executionInfo":{"status":"ok","timestamp":1676719783433,"user_tz":-330,"elapsed":3,"user":{"displayName":"CJ","userId":"16852632714411506132"}},"outputId":"6ce74725-6b82-414a-fe8e-f3453c11c82e"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DeviceArray(0.16572994, dtype=float32)"]},"metadata":{},"execution_count":9}]}]}